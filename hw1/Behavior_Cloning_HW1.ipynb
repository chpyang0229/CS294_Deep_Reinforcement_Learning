{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning:\n",
    "\n",
    "The goal of this notebook is to experiment with imitation learning, in the first part of the assignment, the goal is to set up behavior cloning within OpenAI Gym benchmark suite. The dependent library include:\n",
    "\n",
    "TensorFlow: I used keras to instead the google tensorflow suite.\n",
    "\n",
    "OpenAI Gym\n",
    "\n",
    "MoJoCo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import load_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tf_util\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-18 21:13:19,591] Making new env: Humanoid-v1\n"
     ]
    }
   ],
   "source": [
    "task_name='Humanoid-v1'\n",
    "num_rollouts=6\n",
    "cached_data_path=\"data/\"+task_name+\"-their.p\"\n",
    "their_data_path=\"data/\"+task_name+\"-their.p\"\n",
    "our_data_path=\"data/\"+task_name+\"-our.p\"\n",
    "expert_policy_file=\"experts/\"+task_name+\".pkl\"\n",
    "\n",
    "env=gym.make(task_name)\n",
    "max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "envname=task_name\n",
    "render_them=True\n",
    "render_us=False\n",
    "\n",
    "#neural net params\n",
    "learning_rate=0.001\n",
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_data_table_stats(data):\n",
    "    mean=data['returns'].mean()\n",
    "    std=data['returns'].std()\n",
    "    x=data['steps']\n",
    "    pct_full_steps=(x/x.max()).mean()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'mean reward':mean,\n",
    "        'std reward':std,\n",
    "        'pct full rollout': pct_full_steps\n",
    "    })\n",
    "\n",
    "def view_data(data, rollouts):\n",
    "    returns=[]\n",
    "    observations=[]\n",
    "    actions=[]\n",
    "    print(\"Total rollouts from data:\", rollouts)\n",
    "    env=gym.make(envname)\n",
    "    for i in range(rollouts):\n",
    "        print(\"start rollout\", i)\n",
    "        observation=env.reset()\n",
    "        steps=0\n",
    "        for t in range(2000):\n",
    "            env.render()\n",
    "            x=t+i*max_steps\n",
    "            action=data['actions'][x,:,:]\n",
    "            observations.append(t)\n",
    "            actions.append(action)\n",
    "            observation, reward, done, info=env.step(action)\n",
    "            steps+=1\n",
    "            if steps>=max_steps:\n",
    "                print(\"Max timestep reached\")\n",
    "                break\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n",
    "                \n",
    "def view_model(model,rollouts):\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    env = gym.make(envname)\n",
    "    print (\"Total rollouts from model: \", rollouts)\n",
    "    for i in range(rollouts):\n",
    "        #observation = env.reset()\n",
    "        print (\"Start rollout \", i)\n",
    "        obs = env.reset()\n",
    "        steps = 0\n",
    "        for t in range(2000):\n",
    "            env.render()\n",
    "            #print(observation)\n",
    "            action = model.predict(obs[None, :])\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            steps += 1\n",
    "            if steps >= max_steps:\n",
    "                print(\"Max timestep reached\")\n",
    "                break\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  The part is to get the data of expert policy. \n",
    "the data is usded for traning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "loaded and built\n",
      "Total rollouts for building policy:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:40<00:00, 16.67s/it]\n"
     ]
    }
   ],
   "source": [
    "policy_fn=load_policy.load_policy(expert_policy_file)\n",
    "print('loaded and built')\n",
    "\n",
    "with tf.Session():\n",
    "    tf.variables_initializer\n",
    "    max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    print (\"Total rollouts for building policy: \", num_rollouts)\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    steps_numbers = []\n",
    "    \n",
    "    \n",
    "    for i in tqdm.tqdm(range(num_rollouts)):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        totalr = 0.\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = policy_fn(obs[None,:])\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if render_them:\n",
    "                env.render()\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        steps_numbers.append(steps)\n",
    "        returns.append(totalr)\n",
    "\n",
    "    expert_data = {'observations': np.array(observations),\n",
    "                   'actions': np.array(actions),\n",
    "                   'returns': np.array(returns),\n",
    "                   'steps': np.array(steps_numbers)}\n",
    "\n",
    "pickle.dump(expert_data, open(their_data_path, 'wb'))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the expert policy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pickle.load(open(cached_data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Alternative model designs:\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_inputs/2, input_dim=num_inputs, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_outputs, init='normal'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "def regularized_model():\n",
    "    from keras.regularizers import l2, activity_l2\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=num_inputs, init='normal', activation='relu',W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01),b_regularizer=l2(0.01)))\n",
    "    model.add(Dense(64, input_dim=num_inputs, init='normal', activation='relu',W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01),b_regularizer=l2(0.01)))\n",
    "    model.add(Dense(num_outputs, init='normal',W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01),b_regularizer=l2(0.01)))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "def wide_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=num_inputs, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_outputs, init='normal'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def awesome_model():\n",
    "    model = Sequential([\n",
    "    Lambda(lambda x: (x - mean) / std, batch_input_shape=(None, observations_dim)),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(actions_dim)])\n",
    "\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5400 samples, validate on 600 samples\n",
      "Epoch 1/50\n",
      "0s - loss: 78.1870 - val_loss: 19.1168\n",
      "Epoch 2/50\n",
      "0s - loss: 8.2728 - val_loss: 4.2103\n",
      "Epoch 3/50\n",
      "0s - loss: 2.9558 - val_loss: 2.1356\n",
      "Epoch 4/50\n",
      "0s - loss: 1.8701 - val_loss: 1.6242\n",
      "Epoch 5/50\n",
      "0s - loss: 1.4319 - val_loss: 1.2755\n",
      "Epoch 6/50\n",
      "0s - loss: 1.1748 - val_loss: 1.0865\n",
      "Epoch 7/50\n",
      "0s - loss: 1.0000 - val_loss: 0.9490\n",
      "Epoch 8/50\n",
      "0s - loss: 0.8879 - val_loss: 0.8501\n",
      "Epoch 9/50\n",
      "0s - loss: 0.8084 - val_loss: 0.7742\n",
      "Epoch 10/50\n",
      "0s - loss: 0.7127 - val_loss: 0.7020\n",
      "Epoch 11/50\n",
      "0s - loss: 0.6428 - val_loss: 0.6579\n",
      "Epoch 12/50\n",
      "0s - loss: 0.5967 - val_loss: 0.5972\n",
      "Epoch 13/50\n",
      "0s - loss: 0.5441 - val_loss: 0.5623\n",
      "Epoch 14/50\n",
      "0s - loss: 0.5137 - val_loss: 0.5342\n",
      "Epoch 15/50\n",
      "0s - loss: 0.4762 - val_loss: 0.4951\n",
      "Epoch 16/50\n",
      "0s - loss: 0.4536 - val_loss: 0.4643\n",
      "Epoch 17/50\n",
      "0s - loss: 0.4225 - val_loss: 0.4510\n",
      "Epoch 18/50\n",
      "0s - loss: 0.4041 - val_loss: 0.4285\n",
      "Epoch 19/50\n",
      "0s - loss: 0.3828 - val_loss: 0.4119\n",
      "Epoch 20/50\n",
      "0s - loss: 0.3644 - val_loss: 0.4040\n",
      "Epoch 21/50\n",
      "0s - loss: 0.3543 - val_loss: 0.3818\n",
      "Epoch 22/50\n",
      "0s - loss: 0.3323 - val_loss: 0.3640\n",
      "Epoch 23/50\n",
      "0s - loss: 0.3273 - val_loss: 0.3636\n",
      "Epoch 24/50\n",
      "0s - loss: 0.3125 - val_loss: 0.3462\n",
      "Epoch 25/50\n",
      "0s - loss: 0.3001 - val_loss: 0.3401\n",
      "Epoch 26/50\n",
      "0s - loss: 0.2880 - val_loss: 0.3217\n",
      "Epoch 27/50\n",
      "0s - loss: 0.2781 - val_loss: 0.3174\n",
      "Epoch 28/50\n",
      "0s - loss: 0.2643 - val_loss: 0.2987\n",
      "Epoch 29/50\n",
      "0s - loss: 0.2560 - val_loss: 0.2925\n",
      "Epoch 30/50\n",
      "0s - loss: 0.2574 - val_loss: 0.2837\n",
      "Epoch 31/50\n",
      "0s - loss: 0.2460 - val_loss: 0.2736\n",
      "Epoch 32/50\n",
      "0s - loss: 0.2344 - val_loss: 0.2700\n",
      "Epoch 33/50\n",
      "0s - loss: 0.2267 - val_loss: 0.2712\n",
      "Epoch 34/50\n",
      "0s - loss: 0.2321 - val_loss: 0.2577\n",
      "Epoch 35/50\n",
      "0s - loss: 0.2182 - val_loss: 0.2504\n",
      "Epoch 36/50\n",
      "0s - loss: 0.2130 - val_loss: 0.2614\n",
      "Epoch 37/50\n",
      "0s - loss: 0.2089 - val_loss: 0.2384\n",
      "Epoch 38/50\n",
      "0s - loss: 0.2034 - val_loss: 0.2369\n",
      "Epoch 39/50\n",
      "0s - loss: 0.1971 - val_loss: 0.2384\n",
      "Epoch 40/50\n",
      "0s - loss: 0.1952 - val_loss: 0.2326\n",
      "Epoch 41/50\n",
      "0s - loss: 0.1959 - val_loss: 0.2306\n",
      "Epoch 42/50\n",
      "0s - loss: 0.1906 - val_loss: 0.2250\n",
      "Epoch 43/50\n",
      "0s - loss: 0.1823 - val_loss: 0.2215\n",
      "Epoch 44/50\n",
      "0s - loss: 0.1807 - val_loss: 0.2126\n",
      "Epoch 45/50\n",
      "0s - loss: 0.1789 - val_loss: 0.2310\n",
      "Epoch 46/50\n",
      "0s - loss: 0.1889 - val_loss: 0.2134\n",
      "Epoch 47/50\n",
      "0s - loss: 0.1748 - val_loss: 0.2086\n",
      "Epoch 48/50\n",
      "0s - loss: 0.1706 - val_loss: 0.1959\n",
      "Epoch 49/50\n",
      "0s - loss: 0.1663 - val_loss: 0.2032\n",
      "Epoch 50/50\n",
      "0s - loss: 0.1676 - val_loss: 0.1929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5b9f624e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "mean, std = np.mean(data['observations'], axis=0), np.std(data['observations'], axis=0) + 1e-6\n",
    "observations_dim=env.observation_space.shape[0]\n",
    "actions_dim=env.action_space.shape[0]\n",
    "num_inputs=observations_dim\n",
    "num_outputs=actions_dim\n",
    "\n",
    "model=wide_model()\n",
    "x,y=shuffle(data['observations'],data['actions'].reshape(-1, actions_dim))\n",
    "model.fit(x,y,\n",
    "         validation_split=0.1,\n",
    "         batch_size=256,\n",
    "         nb_epoch=epochs,\n",
    "         verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "returns = []\n",
    "observations = []\n",
    "actions = []\n",
    "steps_numbers = []\n",
    "\n",
    "for i in tqdm.tqdm(range(num_rollouts)):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    totalr = 0.\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = model.predict(obs[None, :])\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        totalr += r\n",
    "        steps += 1\n",
    "        if render_us:\n",
    "            env.render()\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    steps_numbers.append(steps)\n",
    "    returns.append(totalr)\n",
    "\n",
    "our_net_data = {'observations': np.array(observations),\n",
    "                'actions': np.array(actions),\n",
    "                'returns': np.array(returns),\n",
    "                'steps': np.array(steps_numbers)}\n",
    "\n",
    "pickle.dump(our_net_data, open(our_data_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance of two model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the comparsion of expert_policyHumanoid-v1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expert</th>\n",
       "      <th>imitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean reward</th>\n",
       "      <td>10418.305422</td>\n",
       "      <td>186.716306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct full rollout</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.757862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std reward</th>\n",
       "      <td>72.307472</td>\n",
       "      <td>36.592240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        expert   imitation\n",
       "mean reward       10418.305422  186.716306\n",
       "pct full rollout      1.000000    0.757862\n",
       "std reward           72.307472   36.592240"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their=pickle.load(open(their_data_path,'rb'))\n",
    "our=pickle.load(open(our_data_path,'rb'))\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'expert': one_data_table_stats(their),\n",
    "    'imitation': one_data_table_stats(our)\n",
    "})\n",
    "\n",
    "print('the comparsion of expert_policy'+ envname)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animate the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-18 19:56:16,831] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rollouts from data: 5\n",
      "start rollout 0\n",
      "Episode finished after 103 timesteps\n",
      "start rollout 1\n",
      "Episode finished after 90 timesteps\n",
      "start rollout 2\n",
      "Episode finished after 88 timesteps\n",
      "start rollout 3\n",
      "Episode finished after 103 timesteps\n",
      "start rollout 4\n",
      "Episode finished after 90 timesteps\n"
     ]
    }
   ],
   "source": [
    "view_data(data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
